{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b7ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV       # hyperparam selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef  # matric for performance evaluation in training\n",
    "# self-writtten:\n",
    "import data_cleaner as dc  # self-writtten data cleaning functions\n",
    "import ML_func as ml       # self-written ML related functions\n",
    "pd.set_option('display.max_rows', 500)  # let pd.df display() 500 rows\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19275fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\"../clean_data/mci_wv1go_imp.csv\")   # use imputed data\n",
    "x_test = pd.read_csv(\"../clean_data/mci_wv23_imp.csv\")   \n",
    "y_train = x_train.pop(\"progress\")\n",
    "y_test = x_test.pop(\"progress\")\n",
    "x_train, train_imp_mean_sd = dc.standardization(x_train)\n",
    "x_test, test_imp_mean_sd = dc.standardization(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7f5589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_features as int & unique class < 10:\n",
    "cat_cols = x_train.apply(lambda x : (x.dtype==\"int64\") and (len(x.unique())<10) )\n",
    "cat_cols = x_train.columns[cat_cols].to_list()\n",
    "for c in cat_cols:\n",
    "    x_train[c] = x_train[c].astype('category')\n",
    "    x_test[c] = x_test[c].astype('category')\n",
    "\n",
    "\n",
    "# print(x_train[cat_cols].apply(lambda x: x.unique()) )\n",
    "# from sklearn.preprocessing import LabelEncoder  # label encoding\n",
    "# le = LabelEncoder()\n",
    "# x_train.PTRACCAT = le.fit_transform(x_train.PTRACCAT)\n",
    "# x_train.MMSE = le.fit_transform(x_train.MMSE)\n",
    "# print(x_train[cat_cols].apply(lambda x: x.unique()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "083a7512",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_num_leaves</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.253926</td>\n",
       "      <td>0.018139</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'num_leaves': 10, 'reg_...</td>\n",
       "      <td>0.298199</td>\n",
       "      <td>0.472713</td>\n",
       "      <td>0.234868</td>\n",
       "      <td>0.124642</td>\n",
       "      <td>0.555234</td>\n",
       "      <td>0.609438</td>\n",
       "      <td>0.465818</td>\n",
       "      <td>0.639804</td>\n",
       "      <td>0.371455</td>\n",
       "      <td>0.360772</td>\n",
       "      <td>0.413294</td>\n",
       "      <td>0.157838</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.472890</td>\n",
       "      <td>0.074054</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'learning_rate': 0.1, 'num_leaves': 20, 'reg_...</td>\n",
       "      <td>0.216338</td>\n",
       "      <td>0.412306</td>\n",
       "      <td>0.208423</td>\n",
       "      <td>-0.005193</td>\n",
       "      <td>0.484165</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.540878</td>\n",
       "      <td>0.639804</td>\n",
       "      <td>0.525443</td>\n",
       "      <td>0.441637</td>\n",
       "      <td>0.387037</td>\n",
       "      <td>0.183040</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.432723</td>\n",
       "      <td>0.034837</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.05</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.05, 'num_leaves': 31, 'reg...</td>\n",
       "      <td>0.254478</td>\n",
       "      <td>0.376633</td>\n",
       "      <td>0.232627</td>\n",
       "      <td>0.086280</td>\n",
       "      <td>0.484165</td>\n",
       "      <td>0.358891</td>\n",
       "      <td>0.540878</td>\n",
       "      <td>0.513353</td>\n",
       "      <td>0.513353</td>\n",
       "      <td>0.445271</td>\n",
       "      <td>0.380593</td>\n",
       "      <td>0.141689</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.675697</td>\n",
       "      <td>0.097523</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.05</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>{'learning_rate': 0.05, 'num_leaves': 31, 'reg...</td>\n",
       "      <td>0.301244</td>\n",
       "      <td>0.481264</td>\n",
       "      <td>0.264478</td>\n",
       "      <td>0.048073</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.358891</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.639804</td>\n",
       "      <td>0.525443</td>\n",
       "      <td>0.361029</td>\n",
       "      <td>0.379336</td>\n",
       "      <td>0.151935</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.413762</td>\n",
       "      <td>0.038074</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.1</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'num_leaves': 31, 'reg_...</td>\n",
       "      <td>0.266029</td>\n",
       "      <td>0.348635</td>\n",
       "      <td>0.208423</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.437167</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.540878</td>\n",
       "      <td>0.583114</td>\n",
       "      <td>0.513353</td>\n",
       "      <td>0.445271</td>\n",
       "      <td>0.376827</td>\n",
       "      <td>0.163374</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "19       0.253926      0.018139         0.003449        0.000534   \n",
       "21       0.472890      0.074054         0.003833        0.000715   \n",
       "16       0.432723      0.034837         0.004154        0.000681   \n",
       "15       0.675697      0.097523         0.003862        0.000370   \n",
       "25       0.413762      0.038074         0.003895        0.000580   \n",
       "\n",
       "   param_learning_rate param_num_leaves param_reg_lambda  \\\n",
       "19                 0.1               10                1   \n",
       "21                 0.1               20                0   \n",
       "16                0.05               31                1   \n",
       "15                0.05               31                0   \n",
       "25                 0.1               31                1   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "19  {'learning_rate': 0.1, 'num_leaves': 10, 'reg_...           0.298199   \n",
       "21  {'learning_rate': 0.1, 'num_leaves': 20, 'reg_...           0.216338   \n",
       "16  {'learning_rate': 0.05, 'num_leaves': 31, 'reg...           0.254478   \n",
       "15  {'learning_rate': 0.05, 'num_leaves': 31, 'reg...           0.301244   \n",
       "25  {'learning_rate': 0.1, 'num_leaves': 31, 'reg_...           0.266029   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "19           0.472713           0.234868           0.124642   \n",
       "21           0.412306           0.208423          -0.005193   \n",
       "16           0.376633           0.232627           0.086280   \n",
       "15           0.481264           0.264478           0.048073   \n",
       "25           0.348635           0.208423           0.018828   \n",
       "\n",
       "    split4_test_score  split5_test_score  split6_test_score  \\\n",
       "19           0.555234           0.609438           0.465818   \n",
       "21           0.484165           0.406566           0.540878   \n",
       "16           0.484165           0.358891           0.540878   \n",
       "15           0.406566           0.358891           0.406566   \n",
       "25           0.437167           0.406566           0.540878   \n",
       "\n",
       "    split7_test_score  split8_test_score  split9_test_score  mean_test_score  \\\n",
       "19           0.639804           0.371455           0.360772         0.413294   \n",
       "21           0.639804           0.525443           0.441637         0.387037   \n",
       "16           0.513353           0.513353           0.445271         0.380593   \n",
       "15           0.639804           0.525443           0.361029         0.379336   \n",
       "25           0.583114           0.513353           0.445271         0.376827   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "19        0.157838                1  \n",
       "21        0.183040                2  \n",
       "16        0.141689                3  \n",
       "15        0.151935                4  \n",
       "25        0.163374                5  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "# lgb.LGBMClassifier().get_params()   # check default\n",
    "params = {\n",
    "    'num_leaves':[10,20,31],           # key to ctrl overfit\n",
    "    'learning_rate':[0.02, 0.05, 0.1, 0.3],       # learning rate\n",
    "    # 'max_depth':            # leaves more important because leaf-wise growth\n",
    "    # 'n_estimators':[50,100,300],\n",
    "    #'reg_alpha':[0,1,2],   # L1 regularization\n",
    "    'reg_lambda':[0,1,2]   # L2 regularization\n",
    "    \n",
    "}\n",
    "mod_lgb = lgb.LGBMClassifier(boosting_type='gbdt', objective='binary', seed=1)  \n",
    "cv_lgb = GridSearchCV(\n",
    "    estimator = mod_lgb, \n",
    "    param_grid=params,\n",
    "    cv=10, verbose=0,\n",
    "    scoring = 'matthews_corrcoef'  # because outcome imbalanced\n",
    ")\n",
    "cv_lgb.fit(x_train, y_train)       \n",
    "pd.DataFrame(cv_lgb.cv_results_).sort_values(\"rank_test_score\").head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "119b8d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_num_leaves</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.253926</td>\n",
       "      <td>0.018139</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'num_leaves': 10, 'reg_...</td>\n",
       "      <td>0.298199</td>\n",
       "      <td>0.472713</td>\n",
       "      <td>0.234868</td>\n",
       "      <td>0.124642</td>\n",
       "      <td>0.555234</td>\n",
       "      <td>0.609438</td>\n",
       "      <td>0.465818</td>\n",
       "      <td>0.639804</td>\n",
       "      <td>0.371455</td>\n",
       "      <td>0.360772</td>\n",
       "      <td>0.413294</td>\n",
       "      <td>0.157838</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.472890</td>\n",
       "      <td>0.074054</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'learning_rate': 0.1, 'num_leaves': 20, 'reg_...</td>\n",
       "      <td>0.216338</td>\n",
       "      <td>0.412306</td>\n",
       "      <td>0.208423</td>\n",
       "      <td>-0.005193</td>\n",
       "      <td>0.484165</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.540878</td>\n",
       "      <td>0.639804</td>\n",
       "      <td>0.525443</td>\n",
       "      <td>0.441637</td>\n",
       "      <td>0.387037</td>\n",
       "      <td>0.183040</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.432723</td>\n",
       "      <td>0.034837</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.05</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.05, 'num_leaves': 31, 'reg...</td>\n",
       "      <td>0.254478</td>\n",
       "      <td>0.376633</td>\n",
       "      <td>0.232627</td>\n",
       "      <td>0.086280</td>\n",
       "      <td>0.484165</td>\n",
       "      <td>0.358891</td>\n",
       "      <td>0.540878</td>\n",
       "      <td>0.513353</td>\n",
       "      <td>0.513353</td>\n",
       "      <td>0.445271</td>\n",
       "      <td>0.380593</td>\n",
       "      <td>0.141689</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.675697</td>\n",
       "      <td>0.097523</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.05</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>{'learning_rate': 0.05, 'num_leaves': 31, 'reg...</td>\n",
       "      <td>0.301244</td>\n",
       "      <td>0.481264</td>\n",
       "      <td>0.264478</td>\n",
       "      <td>0.048073</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.358891</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.639804</td>\n",
       "      <td>0.525443</td>\n",
       "      <td>0.361029</td>\n",
       "      <td>0.379336</td>\n",
       "      <td>0.151935</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.413762</td>\n",
       "      <td>0.038074</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.1</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'num_leaves': 31, 'reg_...</td>\n",
       "      <td>0.266029</td>\n",
       "      <td>0.348635</td>\n",
       "      <td>0.208423</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.437167</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.540878</td>\n",
       "      <td>0.583114</td>\n",
       "      <td>0.513353</td>\n",
       "      <td>0.445271</td>\n",
       "      <td>0.376827</td>\n",
       "      <td>0.163374</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.399747</td>\n",
       "      <td>0.028323</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.05</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.05, 'num_leaves': 20, 'reg...</td>\n",
       "      <td>0.160036</td>\n",
       "      <td>0.376633</td>\n",
       "      <td>0.280554</td>\n",
       "      <td>0.048073</td>\n",
       "      <td>0.484165</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.540878</td>\n",
       "      <td>0.639804</td>\n",
       "      <td>0.457490</td>\n",
       "      <td>0.361029</td>\n",
       "      <td>0.375523</td>\n",
       "      <td>0.167201</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.677675</td>\n",
       "      <td>0.082241</td>\n",
       "      <td>0.004189</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.1</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>{'learning_rate': 0.1, 'num_leaves': 31, 'reg_...</td>\n",
       "      <td>0.254478</td>\n",
       "      <td>0.442105</td>\n",
       "      <td>0.208423</td>\n",
       "      <td>0.095254</td>\n",
       "      <td>0.319756</td>\n",
       "      <td>0.358891</td>\n",
       "      <td>0.465818</td>\n",
       "      <td>0.639804</td>\n",
       "      <td>0.525443</td>\n",
       "      <td>0.360772</td>\n",
       "      <td>0.367074</td>\n",
       "      <td>0.151302</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.395393</td>\n",
       "      <td>0.025380</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.3</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.3, 'num_leaves': 31, 'reg_...</td>\n",
       "      <td>0.160036</td>\n",
       "      <td>0.407553</td>\n",
       "      <td>0.131250</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.484165</td>\n",
       "      <td>0.437167</td>\n",
       "      <td>0.465818</td>\n",
       "      <td>0.513353</td>\n",
       "      <td>0.480102</td>\n",
       "      <td>0.517815</td>\n",
       "      <td>0.361609</td>\n",
       "      <td>0.175032</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.396820</td>\n",
       "      <td>0.035453</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.3, 'num_leaves': 20, 'reg_...</td>\n",
       "      <td>0.301244</td>\n",
       "      <td>0.376633</td>\n",
       "      <td>0.156086</td>\n",
       "      <td>-0.025831</td>\n",
       "      <td>0.437167</td>\n",
       "      <td>0.319100</td>\n",
       "      <td>0.540878</td>\n",
       "      <td>0.578831</td>\n",
       "      <td>0.457490</td>\n",
       "      <td>0.441637</td>\n",
       "      <td>0.358323</td>\n",
       "      <td>0.172785</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.259001</td>\n",
       "      <td>0.021531</td>\n",
       "      <td>0.003845</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'learning_rate': 0.3, 'num_leaves': 10, 'reg_...</td>\n",
       "      <td>0.298199</td>\n",
       "      <td>0.307988</td>\n",
       "      <td>0.184474</td>\n",
       "      <td>0.086280</td>\n",
       "      <td>0.437167</td>\n",
       "      <td>0.233618</td>\n",
       "      <td>0.540878</td>\n",
       "      <td>0.639804</td>\n",
       "      <td>0.411082</td>\n",
       "      <td>0.441637</td>\n",
       "      <td>0.358113</td>\n",
       "      <td>0.159509</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.463255</td>\n",
       "      <td>0.051703</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.05</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'learning_rate': 0.05, 'num_leaves': 20, 'reg...</td>\n",
       "      <td>0.204833</td>\n",
       "      <td>0.338487</td>\n",
       "      <td>0.234868</td>\n",
       "      <td>0.048073</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.578831</td>\n",
       "      <td>0.588749</td>\n",
       "      <td>0.360772</td>\n",
       "      <td>0.357431</td>\n",
       "      <td>0.156253</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.279607</td>\n",
       "      <td>0.023615</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'learning_rate': 0.1, 'num_leaves': 10, 'reg_...</td>\n",
       "      <td>0.216338</td>\n",
       "      <td>0.372871</td>\n",
       "      <td>0.184474</td>\n",
       "      <td>0.048073</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.358891</td>\n",
       "      <td>0.381227</td>\n",
       "      <td>0.700172</td>\n",
       "      <td>0.457490</td>\n",
       "      <td>0.441637</td>\n",
       "      <td>0.356774</td>\n",
       "      <td>0.168319</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.251225</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.05, 'num_leaves': 10, 'reg...</td>\n",
       "      <td>0.298199</td>\n",
       "      <td>0.441687</td>\n",
       "      <td>0.108858</td>\n",
       "      <td>-0.005193</td>\n",
       "      <td>0.484165</td>\n",
       "      <td>0.319756</td>\n",
       "      <td>0.381227</td>\n",
       "      <td>0.639804</td>\n",
       "      <td>0.525443</td>\n",
       "      <td>0.360772</td>\n",
       "      <td>0.355472</td>\n",
       "      <td>0.181625</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.410234</td>\n",
       "      <td>0.037448</td>\n",
       "      <td>0.003808</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'learning_rate': 0.3, 'num_leaves': 20, 'reg_...</td>\n",
       "      <td>0.204833</td>\n",
       "      <td>0.372871</td>\n",
       "      <td>0.131250</td>\n",
       "      <td>0.086280</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.319100</td>\n",
       "      <td>0.319756</td>\n",
       "      <td>0.697395</td>\n",
       "      <td>0.411082</td>\n",
       "      <td>0.578831</td>\n",
       "      <td>0.352796</td>\n",
       "      <td>0.179099</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.371296</td>\n",
       "      <td>0.033321</td>\n",
       "      <td>0.003717</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.05</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.05, 'num_leaves': 20, 'reg...</td>\n",
       "      <td>0.204833</td>\n",
       "      <td>0.407553</td>\n",
       "      <td>0.156086</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.319756</td>\n",
       "      <td>0.465818</td>\n",
       "      <td>0.465818</td>\n",
       "      <td>0.578831</td>\n",
       "      <td>0.513353</td>\n",
       "      <td>0.361029</td>\n",
       "      <td>0.349190</td>\n",
       "      <td>0.166904</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.245480</td>\n",
       "      <td>0.015056</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.1, 'num_leaves': 10, 'reg_...</td>\n",
       "      <td>0.254478</td>\n",
       "      <td>0.407553</td>\n",
       "      <td>0.338487</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.272254</td>\n",
       "      <td>0.484165</td>\n",
       "      <td>0.381227</td>\n",
       "      <td>0.513353</td>\n",
       "      <td>0.457490</td>\n",
       "      <td>0.360772</td>\n",
       "      <td>0.348861</td>\n",
       "      <td>0.136237</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.523595</td>\n",
       "      <td>0.042830</td>\n",
       "      <td>0.004034</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.3</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>{'learning_rate': 0.3, 'num_leaves': 31, 'reg_...</td>\n",
       "      <td>0.298199</td>\n",
       "      <td>0.376633</td>\n",
       "      <td>0.108858</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.319756</td>\n",
       "      <td>0.319100</td>\n",
       "      <td>0.555234</td>\n",
       "      <td>0.639804</td>\n",
       "      <td>0.480102</td>\n",
       "      <td>0.360772</td>\n",
       "      <td>0.347729</td>\n",
       "      <td>0.177816</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.266088</td>\n",
       "      <td>0.019661</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'learning_rate': 0.05, 'num_leaves': 10, 'reg...</td>\n",
       "      <td>0.124642</td>\n",
       "      <td>0.407553</td>\n",
       "      <td>0.280554</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.465818</td>\n",
       "      <td>0.465818</td>\n",
       "      <td>0.578831</td>\n",
       "      <td>0.457490</td>\n",
       "      <td>0.265216</td>\n",
       "      <td>0.347132</td>\n",
       "      <td>0.164209</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.398473</td>\n",
       "      <td>0.026937</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.3</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.3, 'num_leaves': 31, 'reg_...</td>\n",
       "      <td>0.301244</td>\n",
       "      <td>0.376633</td>\n",
       "      <td>0.156086</td>\n",
       "      <td>-0.005193</td>\n",
       "      <td>0.437167</td>\n",
       "      <td>0.272254</td>\n",
       "      <td>0.484165</td>\n",
       "      <td>0.525443</td>\n",
       "      <td>0.457490</td>\n",
       "      <td>0.441637</td>\n",
       "      <td>0.344693</td>\n",
       "      <td>0.157780</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.405709</td>\n",
       "      <td>0.051194</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.1, 'num_leaves': 20, 'reg_...</td>\n",
       "      <td>0.254478</td>\n",
       "      <td>0.376633</td>\n",
       "      <td>0.047693</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.319756</td>\n",
       "      <td>0.437167</td>\n",
       "      <td>0.540878</td>\n",
       "      <td>0.578831</td>\n",
       "      <td>0.411082</td>\n",
       "      <td>0.445271</td>\n",
       "      <td>0.343062</td>\n",
       "      <td>0.179035</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.262947</td>\n",
       "      <td>0.019001</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.3, 'num_leaves': 10, 'reg_...</td>\n",
       "      <td>0.337423</td>\n",
       "      <td>0.307988</td>\n",
       "      <td>0.234868</td>\n",
       "      <td>0.095254</td>\n",
       "      <td>0.484165</td>\n",
       "      <td>0.233618</td>\n",
       "      <td>0.272254</td>\n",
       "      <td>0.525443</td>\n",
       "      <td>0.411082</td>\n",
       "      <td>0.457490</td>\n",
       "      <td>0.335958</td>\n",
       "      <td>0.127184</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.411460</td>\n",
       "      <td>0.035632</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.1</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.1, 'num_leaves': 31, 'reg_...</td>\n",
       "      <td>0.301244</td>\n",
       "      <td>0.407553</td>\n",
       "      <td>0.069985</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.319100</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.465818</td>\n",
       "      <td>0.457490</td>\n",
       "      <td>0.457490</td>\n",
       "      <td>0.445271</td>\n",
       "      <td>0.334934</td>\n",
       "      <td>0.155324</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.416113</td>\n",
       "      <td>0.023640</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'num_leaves': 20, 'reg_...</td>\n",
       "      <td>0.204833</td>\n",
       "      <td>0.348635</td>\n",
       "      <td>0.234868</td>\n",
       "      <td>-0.025831</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.319100</td>\n",
       "      <td>0.465818</td>\n",
       "      <td>0.578831</td>\n",
       "      <td>0.411082</td>\n",
       "      <td>0.360772</td>\n",
       "      <td>0.330467</td>\n",
       "      <td>0.156698</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.250091</td>\n",
       "      <td>0.020485</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.3, 'num_leaves': 10, 'reg_...</td>\n",
       "      <td>0.216338</td>\n",
       "      <td>0.407553</td>\n",
       "      <td>0.184474</td>\n",
       "      <td>0.086280</td>\n",
       "      <td>0.358891</td>\n",
       "      <td>0.621469</td>\n",
       "      <td>0.272254</td>\n",
       "      <td>0.383257</td>\n",
       "      <td>0.411082</td>\n",
       "      <td>0.360772</td>\n",
       "      <td>0.330237</td>\n",
       "      <td>0.141064</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.404780</td>\n",
       "      <td>0.016986</td>\n",
       "      <td>0.003783</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.3, 'num_leaves': 20, 'reg_...</td>\n",
       "      <td>0.124642</td>\n",
       "      <td>0.376633</td>\n",
       "      <td>0.108858</td>\n",
       "      <td>-0.044121</td>\n",
       "      <td>0.484165</td>\n",
       "      <td>0.358891</td>\n",
       "      <td>0.381227</td>\n",
       "      <td>0.578831</td>\n",
       "      <td>0.480102</td>\n",
       "      <td>0.441637</td>\n",
       "      <td>0.329087</td>\n",
       "      <td>0.189036</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.317253</td>\n",
       "      <td>0.059945</td>\n",
       "      <td>0.004026</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.02</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'learning_rate': 0.02, 'num_leaves': 10, 'reg...</td>\n",
       "      <td>0.253868</td>\n",
       "      <td>0.407553</td>\n",
       "      <td>0.124642</td>\n",
       "      <td>0.086280</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.281347</td>\n",
       "      <td>0.381227</td>\n",
       "      <td>0.517815</td>\n",
       "      <td>0.441637</td>\n",
       "      <td>0.361029</td>\n",
       "      <td>0.326196</td>\n",
       "      <td>0.131450</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.422035</td>\n",
       "      <td>0.055396</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.05</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.05, 'num_leaves': 31, 'reg...</td>\n",
       "      <td>0.266029</td>\n",
       "      <td>0.348635</td>\n",
       "      <td>0.156086</td>\n",
       "      <td>-0.108687</td>\n",
       "      <td>0.319756</td>\n",
       "      <td>0.281347</td>\n",
       "      <td>0.540878</td>\n",
       "      <td>0.578831</td>\n",
       "      <td>0.513353</td>\n",
       "      <td>0.361029</td>\n",
       "      <td>0.325726</td>\n",
       "      <td>0.192455</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.586890</td>\n",
       "      <td>0.064269</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.02</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>{'learning_rate': 0.02, 'num_leaves': 31, 'reg...</td>\n",
       "      <td>0.361475</td>\n",
       "      <td>0.184092</td>\n",
       "      <td>0.156086</td>\n",
       "      <td>0.086280</td>\n",
       "      <td>0.319756</td>\n",
       "      <td>0.281347</td>\n",
       "      <td>0.381227</td>\n",
       "      <td>0.583114</td>\n",
       "      <td>0.457490</td>\n",
       "      <td>0.361029</td>\n",
       "      <td>0.317189</td>\n",
       "      <td>0.140237</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.255268</td>\n",
       "      <td>0.045015</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.05, 'num_leaves': 10, 'reg...</td>\n",
       "      <td>0.204833</td>\n",
       "      <td>0.376633</td>\n",
       "      <td>0.156086</td>\n",
       "      <td>-0.122380</td>\n",
       "      <td>0.406566</td>\n",
       "      <td>0.381227</td>\n",
       "      <td>0.381227</td>\n",
       "      <td>0.383257</td>\n",
       "      <td>0.513353</td>\n",
       "      <td>0.265216</td>\n",
       "      <td>0.294602</td>\n",
       "      <td>0.170871</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.435093</td>\n",
       "      <td>0.051599</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.02</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'learning_rate': 0.02, 'num_leaves': 20, 'reg...</td>\n",
       "      <td>0.266029</td>\n",
       "      <td>0.095254</td>\n",
       "      <td>0.156086</td>\n",
       "      <td>0.086280</td>\n",
       "      <td>0.319756</td>\n",
       "      <td>0.152757</td>\n",
       "      <td>0.381227</td>\n",
       "      <td>0.583114</td>\n",
       "      <td>0.457490</td>\n",
       "      <td>0.361029</td>\n",
       "      <td>0.285902</td>\n",
       "      <td>0.156469</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.250620</td>\n",
       "      <td>0.018030</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.02</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.02, 'num_leaves': 10, 'reg...</td>\n",
       "      <td>0.086280</td>\n",
       "      <td>0.338487</td>\n",
       "      <td>0.160036</td>\n",
       "      <td>-0.093470</td>\n",
       "      <td>0.219096</td>\n",
       "      <td>0.152757</td>\n",
       "      <td>0.381227</td>\n",
       "      <td>0.445271</td>\n",
       "      <td>0.360772</td>\n",
       "      <td>0.361029</td>\n",
       "      <td>0.241149</td>\n",
       "      <td>0.158201</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.373016</td>\n",
       "      <td>0.038435</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.02</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.02, 'num_leaves': 20, 'reg...</td>\n",
       "      <td>0.204833</td>\n",
       "      <td>0.124642</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>-0.075793</td>\n",
       "      <td>0.319756</td>\n",
       "      <td>0.152757</td>\n",
       "      <td>0.281347</td>\n",
       "      <td>0.583114</td>\n",
       "      <td>0.360772</td>\n",
       "      <td>0.253531</td>\n",
       "      <td>0.222379</td>\n",
       "      <td>0.175573</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.375473</td>\n",
       "      <td>0.016990</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.02</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.02, 'num_leaves': 31, 'reg...</td>\n",
       "      <td>0.204833</td>\n",
       "      <td>0.124642</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>-0.075793</td>\n",
       "      <td>0.319756</td>\n",
       "      <td>0.152757</td>\n",
       "      <td>0.281347</td>\n",
       "      <td>0.517815</td>\n",
       "      <td>0.360772</td>\n",
       "      <td>0.253531</td>\n",
       "      <td>0.215849</td>\n",
       "      <td>0.162784</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.247599</td>\n",
       "      <td>0.016586</td>\n",
       "      <td>0.003412</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.02</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.02, 'num_leaves': 10, 'reg...</td>\n",
       "      <td>0.142841</td>\n",
       "      <td>0.264478</td>\n",
       "      <td>0.160036</td>\n",
       "      <td>-0.075793</td>\n",
       "      <td>0.219096</td>\n",
       "      <td>0.152757</td>\n",
       "      <td>0.152757</td>\n",
       "      <td>0.445271</td>\n",
       "      <td>0.265216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172666</td>\n",
       "      <td>0.136694</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.350469</td>\n",
       "      <td>0.028353</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.02</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.02, 'num_leaves': 31, 'reg...</td>\n",
       "      <td>-0.053230</td>\n",
       "      <td>0.264478</td>\n",
       "      <td>0.086280</td>\n",
       "      <td>-0.075793</td>\n",
       "      <td>0.319756</td>\n",
       "      <td>0.152757</td>\n",
       "      <td>0.281347</td>\n",
       "      <td>0.517815</td>\n",
       "      <td>0.142044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163545</td>\n",
       "      <td>0.176506</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.365361</td>\n",
       "      <td>0.033684</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.02</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.02, 'num_leaves': 20, 'reg...</td>\n",
       "      <td>-0.053230</td>\n",
       "      <td>0.264478</td>\n",
       "      <td>0.086280</td>\n",
       "      <td>-0.075793</td>\n",
       "      <td>0.319756</td>\n",
       "      <td>0.152757</td>\n",
       "      <td>0.281347</td>\n",
       "      <td>0.517815</td>\n",
       "      <td>0.142044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163545</td>\n",
       "      <td>0.176506</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "19       0.253926      0.018139         0.003449        0.000534   \n",
       "21       0.472890      0.074054         0.003833        0.000715   \n",
       "16       0.432723      0.034837         0.004154        0.000681   \n",
       "15       0.675697      0.097523         0.003862        0.000370   \n",
       "25       0.413762      0.038074         0.003895        0.000580   \n",
       "13       0.399747      0.028323         0.003564        0.000458   \n",
       "24       0.677675      0.082241         0.004189        0.000754   \n",
       "35       0.395393      0.025380         0.003733        0.000508   \n",
       "31       0.396820      0.035453         0.003591        0.000453   \n",
       "27       0.259001      0.021531         0.003845        0.000515   \n",
       "12       0.463255      0.051703         0.003551        0.000747   \n",
       "18       0.279607      0.023615         0.003677        0.000560   \n",
       "10       0.251225      0.017200         0.003814        0.000710   \n",
       "30       0.410234      0.037448         0.003808        0.000466   \n",
       "14       0.371296      0.033321         0.003717        0.000621   \n",
       "20       0.245480      0.015056         0.003166        0.000280   \n",
       "33       0.523595      0.042830         0.004034        0.000598   \n",
       "9        0.266088      0.019661         0.003626        0.000569   \n",
       "34       0.398473      0.026937         0.003623        0.000495   \n",
       "23       0.405709      0.051194         0.003666        0.000429   \n",
       "28       0.262947      0.019001         0.003501        0.000404   \n",
       "26       0.411460      0.035632         0.003449        0.000347   \n",
       "22       0.416113      0.023640         0.003677        0.000509   \n",
       "29       0.250091      0.020485         0.003521        0.000556   \n",
       "32       0.404780      0.016986         0.003783        0.000449   \n",
       "0        0.317253      0.059945         0.004026        0.001150   \n",
       "17       0.422035      0.055396         0.003848        0.000664   \n",
       "6        0.586890      0.064269         0.004032        0.000659   \n",
       "11       0.255268      0.045015         0.003508        0.000701   \n",
       "3        0.435093      0.051599         0.003411        0.000345   \n",
       "1        0.250620      0.018030         0.003606        0.000453   \n",
       "4        0.373016      0.038435         0.003742        0.000562   \n",
       "7        0.375473      0.016990         0.003814        0.000562   \n",
       "2        0.247599      0.016586         0.003412        0.000781   \n",
       "8        0.350469      0.028353         0.004067        0.001303   \n",
       "5        0.365361      0.033684         0.003892        0.000457   \n",
       "\n",
       "   param_learning_rate param_num_leaves param_reg_lambda  \\\n",
       "19                 0.1               10                1   \n",
       "21                 0.1               20                0   \n",
       "16                0.05               31                1   \n",
       "15                0.05               31                0   \n",
       "25                 0.1               31                1   \n",
       "13                0.05               20                1   \n",
       "24                 0.1               31                0   \n",
       "35                 0.3               31                2   \n",
       "31                 0.3               20                1   \n",
       "27                 0.3               10                0   \n",
       "12                0.05               20                0   \n",
       "18                 0.1               10                0   \n",
       "10                0.05               10                1   \n",
       "30                 0.3               20                0   \n",
       "14                0.05               20                2   \n",
       "20                 0.1               10                2   \n",
       "33                 0.3               31                0   \n",
       "9                 0.05               10                0   \n",
       "34                 0.3               31                1   \n",
       "23                 0.1               20                2   \n",
       "28                 0.3               10                1   \n",
       "26                 0.1               31                2   \n",
       "22                 0.1               20                1   \n",
       "29                 0.3               10                2   \n",
       "32                 0.3               20                2   \n",
       "0                 0.02               10                0   \n",
       "17                0.05               31                2   \n",
       "6                 0.02               31                0   \n",
       "11                0.05               10                2   \n",
       "3                 0.02               20                0   \n",
       "1                 0.02               10                1   \n",
       "4                 0.02               20                1   \n",
       "7                 0.02               31                1   \n",
       "2                 0.02               10                2   \n",
       "8                 0.02               31                2   \n",
       "5                 0.02               20                2   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "19  {'learning_rate': 0.1, 'num_leaves': 10, 'reg_...           0.298199   \n",
       "21  {'learning_rate': 0.1, 'num_leaves': 20, 'reg_...           0.216338   \n",
       "16  {'learning_rate': 0.05, 'num_leaves': 31, 'reg...           0.254478   \n",
       "15  {'learning_rate': 0.05, 'num_leaves': 31, 'reg...           0.301244   \n",
       "25  {'learning_rate': 0.1, 'num_leaves': 31, 'reg_...           0.266029   \n",
       "13  {'learning_rate': 0.05, 'num_leaves': 20, 'reg...           0.160036   \n",
       "24  {'learning_rate': 0.1, 'num_leaves': 31, 'reg_...           0.254478   \n",
       "35  {'learning_rate': 0.3, 'num_leaves': 31, 'reg_...           0.160036   \n",
       "31  {'learning_rate': 0.3, 'num_leaves': 20, 'reg_...           0.301244   \n",
       "27  {'learning_rate': 0.3, 'num_leaves': 10, 'reg_...           0.298199   \n",
       "12  {'learning_rate': 0.05, 'num_leaves': 20, 'reg...           0.204833   \n",
       "18  {'learning_rate': 0.1, 'num_leaves': 10, 'reg_...           0.216338   \n",
       "10  {'learning_rate': 0.05, 'num_leaves': 10, 'reg...           0.298199   \n",
       "30  {'learning_rate': 0.3, 'num_leaves': 20, 'reg_...           0.204833   \n",
       "14  {'learning_rate': 0.05, 'num_leaves': 20, 'reg...           0.204833   \n",
       "20  {'learning_rate': 0.1, 'num_leaves': 10, 'reg_...           0.254478   \n",
       "33  {'learning_rate': 0.3, 'num_leaves': 31, 'reg_...           0.298199   \n",
       "9   {'learning_rate': 0.05, 'num_leaves': 10, 'reg...           0.124642   \n",
       "34  {'learning_rate': 0.3, 'num_leaves': 31, 'reg_...           0.301244   \n",
       "23  {'learning_rate': 0.1, 'num_leaves': 20, 'reg_...           0.254478   \n",
       "28  {'learning_rate': 0.3, 'num_leaves': 10, 'reg_...           0.337423   \n",
       "26  {'learning_rate': 0.1, 'num_leaves': 31, 'reg_...           0.301244   \n",
       "22  {'learning_rate': 0.1, 'num_leaves': 20, 'reg_...           0.204833   \n",
       "29  {'learning_rate': 0.3, 'num_leaves': 10, 'reg_...           0.216338   \n",
       "32  {'learning_rate': 0.3, 'num_leaves': 20, 'reg_...           0.124642   \n",
       "0   {'learning_rate': 0.02, 'num_leaves': 10, 'reg...           0.253868   \n",
       "17  {'learning_rate': 0.05, 'num_leaves': 31, 'reg...           0.266029   \n",
       "6   {'learning_rate': 0.02, 'num_leaves': 31, 'reg...           0.361475   \n",
       "11  {'learning_rate': 0.05, 'num_leaves': 10, 'reg...           0.204833   \n",
       "3   {'learning_rate': 0.02, 'num_leaves': 20, 'reg...           0.266029   \n",
       "1   {'learning_rate': 0.02, 'num_leaves': 10, 'reg...           0.086280   \n",
       "4   {'learning_rate': 0.02, 'num_leaves': 20, 'reg...           0.204833   \n",
       "7   {'learning_rate': 0.02, 'num_leaves': 31, 'reg...           0.204833   \n",
       "2   {'learning_rate': 0.02, 'num_leaves': 10, 'reg...           0.142841   \n",
       "8   {'learning_rate': 0.02, 'num_leaves': 31, 'reg...          -0.053230   \n",
       "5   {'learning_rate': 0.02, 'num_leaves': 20, 'reg...          -0.053230   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "19           0.472713           0.234868           0.124642   \n",
       "21           0.412306           0.208423          -0.005193   \n",
       "16           0.376633           0.232627           0.086280   \n",
       "15           0.481264           0.264478           0.048073   \n",
       "25           0.348635           0.208423           0.018828   \n",
       "13           0.376633           0.280554           0.048073   \n",
       "24           0.442105           0.208423           0.095254   \n",
       "35           0.407553           0.131250           0.018828   \n",
       "31           0.376633           0.156086          -0.025831   \n",
       "27           0.307988           0.184474           0.086280   \n",
       "12           0.338487           0.234868           0.048073   \n",
       "18           0.372871           0.184474           0.048073   \n",
       "10           0.441687           0.108858          -0.005193   \n",
       "30           0.372871           0.131250           0.086280   \n",
       "14           0.407553           0.156086           0.018828   \n",
       "20           0.407553           0.338487           0.018828   \n",
       "33           0.376633           0.108858           0.018828   \n",
       "9            0.407553           0.280554           0.018828   \n",
       "34           0.376633           0.156086          -0.005193   \n",
       "23           0.376633           0.047693           0.018828   \n",
       "28           0.307988           0.234868           0.095254   \n",
       "26           0.407553           0.069985           0.018828   \n",
       "22           0.348635           0.234868          -0.025831   \n",
       "29           0.407553           0.184474           0.086280   \n",
       "32           0.376633           0.108858          -0.044121   \n",
       "0            0.407553           0.124642           0.086280   \n",
       "17           0.348635           0.156086          -0.108687   \n",
       "6            0.184092           0.156086           0.086280   \n",
       "11           0.376633           0.156086          -0.122380   \n",
       "3            0.095254           0.156086           0.086280   \n",
       "1            0.338487           0.160036          -0.093470   \n",
       "4            0.124642           0.018828          -0.075793   \n",
       "7            0.124642           0.018828          -0.075793   \n",
       "2            0.264478           0.160036          -0.075793   \n",
       "8            0.264478           0.086280          -0.075793   \n",
       "5            0.264478           0.086280          -0.075793   \n",
       "\n",
       "    split4_test_score  split5_test_score  split6_test_score  \\\n",
       "19           0.555234           0.609438           0.465818   \n",
       "21           0.484165           0.406566           0.540878   \n",
       "16           0.484165           0.358891           0.540878   \n",
       "15           0.406566           0.358891           0.406566   \n",
       "25           0.437167           0.406566           0.540878   \n",
       "13           0.484165           0.406566           0.540878   \n",
       "24           0.319756           0.358891           0.465818   \n",
       "35           0.484165           0.437167           0.465818   \n",
       "31           0.437167           0.319100           0.540878   \n",
       "27           0.437167           0.233618           0.540878   \n",
       "12           0.406566           0.406566           0.406566   \n",
       "18           0.406566           0.358891           0.381227   \n",
       "10           0.484165           0.319756           0.381227   \n",
       "30           0.406566           0.319100           0.319756   \n",
       "14           0.319756           0.465818           0.465818   \n",
       "20           0.272254           0.484165           0.381227   \n",
       "33           0.319756           0.319100           0.555234   \n",
       "9            0.406566           0.465818           0.465818   \n",
       "34           0.437167           0.272254           0.484165   \n",
       "23           0.319756           0.437167           0.540878   \n",
       "28           0.484165           0.233618           0.272254   \n",
       "26           0.319100           0.406566           0.465818   \n",
       "22           0.406566           0.319100           0.465818   \n",
       "29           0.358891           0.621469           0.272254   \n",
       "32           0.484165           0.358891           0.381227   \n",
       "0            0.406566           0.281347           0.381227   \n",
       "17           0.319756           0.281347           0.540878   \n",
       "6            0.319756           0.281347           0.381227   \n",
       "11           0.406566           0.381227           0.381227   \n",
       "3            0.319756           0.152757           0.381227   \n",
       "1            0.219096           0.152757           0.381227   \n",
       "4            0.319756           0.152757           0.281347   \n",
       "7            0.319756           0.152757           0.281347   \n",
       "2            0.219096           0.152757           0.152757   \n",
       "8            0.319756           0.152757           0.281347   \n",
       "5            0.319756           0.152757           0.281347   \n",
       "\n",
       "    split7_test_score  split8_test_score  split9_test_score  mean_test_score  \\\n",
       "19           0.639804           0.371455           0.360772         0.413294   \n",
       "21           0.639804           0.525443           0.441637         0.387037   \n",
       "16           0.513353           0.513353           0.445271         0.380593   \n",
       "15           0.639804           0.525443           0.361029         0.379336   \n",
       "25           0.583114           0.513353           0.445271         0.376827   \n",
       "13           0.639804           0.457490           0.361029         0.375523   \n",
       "24           0.639804           0.525443           0.360772         0.367074   \n",
       "35           0.513353           0.480102           0.517815         0.361609   \n",
       "31           0.578831           0.457490           0.441637         0.358323   \n",
       "27           0.639804           0.411082           0.441637         0.358113   \n",
       "12           0.578831           0.588749           0.360772         0.357431   \n",
       "18           0.700172           0.457490           0.441637         0.356774   \n",
       "10           0.639804           0.525443           0.360772         0.355472   \n",
       "30           0.697395           0.411082           0.578831         0.352796   \n",
       "14           0.578831           0.513353           0.361029         0.349190   \n",
       "20           0.513353           0.457490           0.360772         0.348861   \n",
       "33           0.639804           0.480102           0.360772         0.347729   \n",
       "9            0.578831           0.457490           0.265216         0.347132   \n",
       "34           0.525443           0.457490           0.441637         0.344693   \n",
       "23           0.578831           0.411082           0.445271         0.343062   \n",
       "28           0.525443           0.411082           0.457490         0.335958   \n",
       "26           0.457490           0.457490           0.445271         0.334934   \n",
       "22           0.578831           0.411082           0.360772         0.330467   \n",
       "29           0.383257           0.411082           0.360772         0.330237   \n",
       "32           0.578831           0.480102           0.441637         0.329087   \n",
       "0            0.517815           0.441637           0.361029         0.326196   \n",
       "17           0.578831           0.513353           0.361029         0.325726   \n",
       "6            0.583114           0.457490           0.361029         0.317189   \n",
       "11           0.383257           0.513353           0.265216         0.294602   \n",
       "3            0.583114           0.457490           0.361029         0.285902   \n",
       "1            0.445271           0.360772           0.361029         0.241149   \n",
       "4            0.583114           0.360772           0.253531         0.222379   \n",
       "7            0.517815           0.360772           0.253531         0.215849   \n",
       "2            0.445271           0.265216           0.000000         0.172666   \n",
       "8            0.517815           0.142044           0.000000         0.163545   \n",
       "5            0.517815           0.142044           0.000000         0.163545   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "19        0.157838                1  \n",
       "21        0.183040                2  \n",
       "16        0.141689                3  \n",
       "15        0.151935                4  \n",
       "25        0.163374                5  \n",
       "13        0.167201                6  \n",
       "24        0.151302                7  \n",
       "35        0.175032                8  \n",
       "31        0.172785                9  \n",
       "27        0.159509               10  \n",
       "12        0.156253               11  \n",
       "18        0.168319               12  \n",
       "10        0.181625               13  \n",
       "30        0.179099               14  \n",
       "14        0.166904               15  \n",
       "20        0.136237               16  \n",
       "33        0.177816               17  \n",
       "9         0.164209               18  \n",
       "34        0.157780               19  \n",
       "23        0.179035               20  \n",
       "28        0.127184               21  \n",
       "26        0.155324               22  \n",
       "22        0.156698               23  \n",
       "29        0.141064               24  \n",
       "32        0.189036               25  \n",
       "0         0.131450               26  \n",
       "17        0.192455               27  \n",
       "6         0.140237               28  \n",
       "11        0.170871               29  \n",
       "3         0.156469               30  \n",
       "1         0.158201               31  \n",
       "4         0.175573               32  \n",
       "7         0.162784               33  \n",
       "2         0.136694               34  \n",
       "8         0.176506               35  \n",
       "5         0.176506               35  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cv_lgb.cv_results_).sort_values(\"rank_test_score\")\n",
    "# ml.vote_hyperparam(cv_lgb.cv_results_, top_n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a2a72159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[1]\ttraining's binary_logloss: 0.427442\tvalid_1's binary_logloss: 0.434558\n",
      "[2]\ttraining's binary_logloss: 0.401624\tvalid_1's binary_logloss: 0.41776\n",
      "[3]\ttraining's binary_logloss: 0.379618\tvalid_1's binary_logloss: 0.407135\n",
      "[4]\ttraining's binary_logloss: 0.360896\tvalid_1's binary_logloss: 0.397806\n",
      "[5]\ttraining's binary_logloss: 0.342703\tvalid_1's binary_logloss: 0.389745\n",
      "[6]\ttraining's binary_logloss: 0.3279\tvalid_1's binary_logloss: 0.385104\n",
      "[7]\ttraining's binary_logloss: 0.313083\tvalid_1's binary_logloss: 0.380147\n",
      "[8]\ttraining's binary_logloss: 0.299837\tvalid_1's binary_logloss: 0.37565\n",
      "[9]\ttraining's binary_logloss: 0.288648\tvalid_1's binary_logloss: 0.371072\n",
      "[10]\ttraining's binary_logloss: 0.276477\tvalid_1's binary_logloss: 0.366956\n",
      "[11]\ttraining's binary_logloss: 0.265553\tvalid_1's binary_logloss: 0.364711\n",
      "[12]\ttraining's binary_logloss: 0.255315\tvalid_1's binary_logloss: 0.362551\n",
      "[13]\ttraining's binary_logloss: 0.245444\tvalid_1's binary_logloss: 0.36007\n",
      "[14]\ttraining's binary_logloss: 0.236753\tvalid_1's binary_logloss: 0.361551\n",
      "[15]\ttraining's binary_logloss: 0.228124\tvalid_1's binary_logloss: 0.359574\n",
      "[16]\ttraining's binary_logloss: 0.220241\tvalid_1's binary_logloss: 0.358381\n",
      "[17]\ttraining's binary_logloss: 0.211827\tvalid_1's binary_logloss: 0.359537\n",
      "[18]\ttraining's binary_logloss: 0.202985\tvalid_1's binary_logloss: 0.357768\n",
      "[19]\ttraining's binary_logloss: 0.196799\tvalid_1's binary_logloss: 0.355596\n",
      "[20]\ttraining's binary_logloss: 0.189277\tvalid_1's binary_logloss: 0.357513\n",
      "[21]\ttraining's binary_logloss: 0.182526\tvalid_1's binary_logloss: 0.359605\n",
      "[22]\ttraining's binary_logloss: 0.176343\tvalid_1's binary_logloss: 0.361183\n",
      "[23]\ttraining's binary_logloss: 0.170747\tvalid_1's binary_logloss: 0.361691\n",
      "[24]\ttraining's binary_logloss: 0.165233\tvalid_1's binary_logloss: 0.362448\n",
      "[25]\ttraining's binary_logloss: 0.160524\tvalid_1's binary_logloss: 0.36133\n",
      "[26]\ttraining's binary_logloss: 0.154985\tvalid_1's binary_logloss: 0.36161\n",
      "[27]\ttraining's binary_logloss: 0.149074\tvalid_1's binary_logloss: 0.361992\n",
      "[28]\ttraining's binary_logloss: 0.144291\tvalid_1's binary_logloss: 0.361476\n",
      "[29]\ttraining's binary_logloss: 0.139442\tvalid_1's binary_logloss: 0.361432\n",
      "[30]\ttraining's binary_logloss: 0.135584\tvalid_1's binary_logloss: 0.3605\n",
      "[31]\ttraining's binary_logloss: 0.131165\tvalid_1's binary_logloss: 0.359782\n",
      "[32]\ttraining's binary_logloss: 0.126654\tvalid_1's binary_logloss: 0.361006\n",
      "[33]\ttraining's binary_logloss: 0.122732\tvalid_1's binary_logloss: 0.360433\n",
      "[34]\ttraining's binary_logloss: 0.119021\tvalid_1's binary_logloss: 0.360141\n",
      "[35]\ttraining's binary_logloss: 0.115559\tvalid_1's binary_logloss: 0.361389\n",
      "[36]\ttraining's binary_logloss: 0.111831\tvalid_1's binary_logloss: 0.36334\n",
      "[37]\ttraining's binary_logloss: 0.108614\tvalid_1's binary_logloss: 0.3621\n",
      "[38]\ttraining's binary_logloss: 0.10509\tvalid_1's binary_logloss: 0.362329\n",
      "[39]\ttraining's binary_logloss: 0.101413\tvalid_1's binary_logloss: 0.361912\n",
      "[40]\ttraining's binary_logloss: 0.0980842\tvalid_1's binary_logloss: 0.360796\n",
      "[41]\ttraining's binary_logloss: 0.0946266\tvalid_1's binary_logloss: 0.361282\n",
      "[42]\ttraining's binary_logloss: 0.0917359\tvalid_1's binary_logloss: 0.360352\n",
      "[43]\ttraining's binary_logloss: 0.0883504\tvalid_1's binary_logloss: 0.361085\n",
      "[44]\ttraining's binary_logloss: 0.0858802\tvalid_1's binary_logloss: 0.362812\n",
      "[45]\ttraining's binary_logloss: 0.0834708\tvalid_1's binary_logloss: 0.364068\n",
      "[46]\ttraining's binary_logloss: 0.0812497\tvalid_1's binary_logloss: 0.364665\n",
      "[47]\ttraining's binary_logloss: 0.0786815\tvalid_1's binary_logloss: 0.365792\n",
      "[48]\ttraining's binary_logloss: 0.0763276\tvalid_1's binary_logloss: 0.366899\n",
      "[49]\ttraining's binary_logloss: 0.0741092\tvalid_1's binary_logloss: 0.366525\n",
      "[50]\ttraining's binary_logloss: 0.0716673\tvalid_1's binary_logloss: 0.369881\n",
      "[51]\ttraining's binary_logloss: 0.0698687\tvalid_1's binary_logloss: 0.37288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/boost/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/opt/anaconda3/envs/boost/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52]\ttraining's binary_logloss: 0.0678469\tvalid_1's binary_logloss: 0.375893\n",
      "[53]\ttraining's binary_logloss: 0.0658528\tvalid_1's binary_logloss: 0.376113\n",
      "[54]\ttraining's binary_logloss: 0.0642285\tvalid_1's binary_logloss: 0.378566\n",
      "[55]\ttraining's binary_logloss: 0.0625609\tvalid_1's binary_logloss: 0.379444\n",
      "[56]\ttraining's binary_logloss: 0.0605571\tvalid_1's binary_logloss: 0.381363\n",
      "[57]\ttraining's binary_logloss: 0.0591382\tvalid_1's binary_logloss: 0.38345\n",
      "[58]\ttraining's binary_logloss: 0.0574436\tvalid_1's binary_logloss: 0.385452\n",
      "[59]\ttraining's binary_logloss: 0.0561149\tvalid_1's binary_logloss: 0.388395\n",
      "[60]\ttraining's binary_logloss: 0.0548123\tvalid_1's binary_logloss: 0.388694\n",
      "[61]\ttraining's binary_logloss: 0.0533352\tvalid_1's binary_logloss: 0.389823\n",
      "[62]\ttraining's binary_logloss: 0.0519754\tvalid_1's binary_logloss: 0.390174\n",
      "[63]\ttraining's binary_logloss: 0.0505017\tvalid_1's binary_logloss: 0.392583\n",
      "[64]\ttraining's binary_logloss: 0.0489661\tvalid_1's binary_logloss: 0.396064\n",
      "[65]\ttraining's binary_logloss: 0.0477238\tvalid_1's binary_logloss: 0.39848\n",
      "[66]\ttraining's binary_logloss: 0.0464798\tvalid_1's binary_logloss: 0.39985\n",
      "[67]\ttraining's binary_logloss: 0.0454025\tvalid_1's binary_logloss: 0.402432\n",
      "[68]\ttraining's binary_logloss: 0.0443068\tvalid_1's binary_logloss: 0.402695\n",
      "[69]\ttraining's binary_logloss: 0.0432728\tvalid_1's binary_logloss: 0.40569\n",
      "[1]\ttraining's binary_logloss: 0.429132\tvalid_1's binary_logloss: 0.434035\n",
      "[2]\ttraining's binary_logloss: 0.40292\tvalid_1's binary_logloss: 0.419912\n",
      "[3]\ttraining's binary_logloss: 0.382038\tvalid_1's binary_logloss: 0.409566\n",
      "[4]\ttraining's binary_logloss: 0.363304\tvalid_1's binary_logloss: 0.398934\n",
      "[5]\ttraining's binary_logloss: 0.346576\tvalid_1's binary_logloss: 0.389104\n",
      "[6]\ttraining's binary_logloss: 0.332015\tvalid_1's binary_logloss: 0.380523\n",
      "[7]\ttraining's binary_logloss: 0.317778\tvalid_1's binary_logloss: 0.370604\n",
      "[8]\ttraining's binary_logloss: 0.303838\tvalid_1's binary_logloss: 0.36819\n",
      "[9]\ttraining's binary_logloss: 0.290998\tvalid_1's binary_logloss: 0.363854\n",
      "[10]\ttraining's binary_logloss: 0.279586\tvalid_1's binary_logloss: 0.360155\n",
      "[11]\ttraining's binary_logloss: 0.268755\tvalid_1's binary_logloss: 0.35741\n",
      "[12]\ttraining's binary_logloss: 0.257836\tvalid_1's binary_logloss: 0.352253\n",
      "[13]\ttraining's binary_logloss: 0.24907\tvalid_1's binary_logloss: 0.34938\n",
      "[14]\ttraining's binary_logloss: 0.240284\tvalid_1's binary_logloss: 0.345985\n",
      "[15]\ttraining's binary_logloss: 0.230527\tvalid_1's binary_logloss: 0.344891\n",
      "[16]\ttraining's binary_logloss: 0.222497\tvalid_1's binary_logloss: 0.342519\n",
      "[17]\ttraining's binary_logloss: 0.214375\tvalid_1's binary_logloss: 0.34269\n",
      "[18]\ttraining's binary_logloss: 0.207105\tvalid_1's binary_logloss: 0.34076\n",
      "[19]\ttraining's binary_logloss: 0.200724\tvalid_1's binary_logloss: 0.33732\n",
      "[20]\ttraining's binary_logloss: 0.193808\tvalid_1's binary_logloss: 0.335446\n",
      "[21]\ttraining's binary_logloss: 0.186926\tvalid_1's binary_logloss: 0.334656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/boost/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/opt/anaconda3/envs/boost/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22]\ttraining's binary_logloss: 0.180255\tvalid_1's binary_logloss: 0.334911\n",
      "[23]\ttraining's binary_logloss: 0.175185\tvalid_1's binary_logloss: 0.334273\n",
      "[24]\ttraining's binary_logloss: 0.169007\tvalid_1's binary_logloss: 0.336612\n",
      "[25]\ttraining's binary_logloss: 0.163775\tvalid_1's binary_logloss: 0.33665\n",
      "[26]\ttraining's binary_logloss: 0.157646\tvalid_1's binary_logloss: 0.336931\n",
      "[27]\ttraining's binary_logloss: 0.153444\tvalid_1's binary_logloss: 0.334458\n",
      "[28]\ttraining's binary_logloss: 0.148127\tvalid_1's binary_logloss: 0.335511\n",
      "[29]\ttraining's binary_logloss: 0.142919\tvalid_1's binary_logloss: 0.335979\n",
      "[30]\ttraining's binary_logloss: 0.13775\tvalid_1's binary_logloss: 0.334311\n",
      "[31]\ttraining's binary_logloss: 0.133031\tvalid_1's binary_logloss: 0.333077\n",
      "[32]\ttraining's binary_logloss: 0.128207\tvalid_1's binary_logloss: 0.335102\n",
      "[33]\ttraining's binary_logloss: 0.12392\tvalid_1's binary_logloss: 0.333048\n",
      "[34]\ttraining's binary_logloss: 0.120114\tvalid_1's binary_logloss: 0.334662\n",
      "[35]\ttraining's binary_logloss: 0.116518\tvalid_1's binary_logloss: 0.336407\n",
      "[36]\ttraining's binary_logloss: 0.112702\tvalid_1's binary_logloss: 0.337749\n",
      "[37]\ttraining's binary_logloss: 0.109967\tvalid_1's binary_logloss: 0.334334\n",
      "[38]\ttraining's binary_logloss: 0.106508\tvalid_1's binary_logloss: 0.335937\n",
      "[39]\ttraining's binary_logloss: 0.103106\tvalid_1's binary_logloss: 0.335753\n",
      "[40]\ttraining's binary_logloss: 0.100076\tvalid_1's binary_logloss: 0.337241\n",
      "[41]\ttraining's binary_logloss: 0.0969014\tvalid_1's binary_logloss: 0.339575\n",
      "[42]\ttraining's binary_logloss: 0.0938394\tvalid_1's binary_logloss: 0.342654\n",
      "[43]\ttraining's binary_logloss: 0.0907871\tvalid_1's binary_logloss: 0.340754\n",
      "[44]\ttraining's binary_logloss: 0.0886432\tvalid_1's binary_logloss: 0.3376\n",
      "[45]\ttraining's binary_logloss: 0.0855722\tvalid_1's binary_logloss: 0.337176\n",
      "[46]\ttraining's binary_logloss: 0.0830171\tvalid_1's binary_logloss: 0.33975\n",
      "[47]\ttraining's binary_logloss: 0.0808697\tvalid_1's binary_logloss: 0.3391\n",
      "[48]\ttraining's binary_logloss: 0.0784896\tvalid_1's binary_logloss: 0.337084\n",
      "[49]\ttraining's binary_logloss: 0.0762569\tvalid_1's binary_logloss: 0.337681\n",
      "[50]\ttraining's binary_logloss: 0.0738688\tvalid_1's binary_logloss: 0.338734\n",
      "[51]\ttraining's binary_logloss: 0.0719767\tvalid_1's binary_logloss: 0.339966\n",
      "[52]\ttraining's binary_logloss: 0.0697937\tvalid_1's binary_logloss: 0.341011\n",
      "[53]\ttraining's binary_logloss: 0.0678062\tvalid_1's binary_logloss: 0.34154\n",
      "[54]\ttraining's binary_logloss: 0.0661987\tvalid_1's binary_logloss: 0.340832\n",
      "[55]\ttraining's binary_logloss: 0.0642206\tvalid_1's binary_logloss: 0.342288\n",
      "[56]\ttraining's binary_logloss: 0.0624092\tvalid_1's binary_logloss: 0.342722\n",
      "[57]\ttraining's binary_logloss: 0.060589\tvalid_1's binary_logloss: 0.341031\n",
      "[58]\ttraining's binary_logloss: 0.0590367\tvalid_1's binary_logloss: 0.342234\n",
      "[59]\ttraining's binary_logloss: 0.0575168\tvalid_1's binary_logloss: 0.343739\n",
      "[60]\ttraining's binary_logloss: 0.0562204\tvalid_1's binary_logloss: 0.345144\n",
      "[61]\ttraining's binary_logloss: 0.0548848\tvalid_1's binary_logloss: 0.346214\n",
      "[62]\ttraining's binary_logloss: 0.053504\tvalid_1's binary_logloss: 0.34567\n",
      "[63]\ttraining's binary_logloss: 0.0521747\tvalid_1's binary_logloss: 0.345637\n",
      "[64]\ttraining's binary_logloss: 0.0507508\tvalid_1's binary_logloss: 0.347428\n",
      "[65]\ttraining's binary_logloss: 0.0493862\tvalid_1's binary_logloss: 0.350953\n",
      "[66]\ttraining's binary_logloss: 0.048209\tvalid_1's binary_logloss: 0.349674\n",
      "[67]\ttraining's binary_logloss: 0.0469875\tvalid_1's binary_logloss: 0.350024\n",
      "[68]\ttraining's binary_logloss: 0.0459456\tvalid_1's binary_logloss: 0.349872\n",
      "[69]\ttraining's binary_logloss: 0.044752\tvalid_1's binary_logloss: 0.351761\n",
      "[70]\ttraining's binary_logloss: 0.0436991\tvalid_1's binary_logloss: 0.352799\n",
      "[71]\ttraining's binary_logloss: 0.0427961\tvalid_1's binary_logloss: 0.352932\n",
      "[72]\ttraining's binary_logloss: 0.0418249\tvalid_1's binary_logloss: 0.352128\n",
      "[73]\ttraining's binary_logloss: 0.0407059\tvalid_1's binary_logloss: 0.352625\n",
      "[74]\ttraining's binary_logloss: 0.0398203\tvalid_1's binary_logloss: 0.352454\n",
      "[75]\ttraining's binary_logloss: 0.0388949\tvalid_1's binary_logloss: 0.353993\n",
      "[76]\ttraining's binary_logloss: 0.0380864\tvalid_1's binary_logloss: 0.358049\n",
      "[77]\ttraining's binary_logloss: 0.0372352\tvalid_1's binary_logloss: 0.356789\n",
      "[78]\ttraining's binary_logloss: 0.0363331\tvalid_1's binary_logloss: 0.358263\n",
      "[79]\ttraining's binary_logloss: 0.0355778\tvalid_1's binary_logloss: 0.359752\n",
      "[80]\ttraining's binary_logloss: 0.034736\tvalid_1's binary_logloss: 0.361776\n",
      "[81]\ttraining's binary_logloss: 0.0339535\tvalid_1's binary_logloss: 0.361981\n",
      "[82]\ttraining's binary_logloss: 0.0331223\tvalid_1's binary_logloss: 0.36397\n",
      "[83]\ttraining's binary_logloss: 0.0325083\tvalid_1's binary_logloss: 0.365365\n",
      "[1]\ttraining's binary_logloss: 0.427706\tvalid_1's binary_logloss: 0.434802\n",
      "[2]\ttraining's binary_logloss: 0.402477\tvalid_1's binary_logloss: 0.416771\n",
      "[3]\ttraining's binary_logloss: 0.379002\tvalid_1's binary_logloss: 0.407517\n",
      "[4]\ttraining's binary_logloss: 0.359866\tvalid_1's binary_logloss: 0.400108\n",
      "[5]\ttraining's binary_logloss: 0.341295\tvalid_1's binary_logloss: 0.392381\n",
      "[6]\ttraining's binary_logloss: 0.324361\tvalid_1's binary_logloss: 0.386574\n",
      "[7]\ttraining's binary_logloss: 0.310198\tvalid_1's binary_logloss: 0.380366\n",
      "[8]\ttraining's binary_logloss: 0.296804\tvalid_1's binary_logloss: 0.373245\n",
      "[9]\ttraining's binary_logloss: 0.282869\tvalid_1's binary_logloss: 0.370415\n",
      "[10]\ttraining's binary_logloss: 0.271505\tvalid_1's binary_logloss: 0.367165\n",
      "[11]\ttraining's binary_logloss: 0.260742\tvalid_1's binary_logloss: 0.363699\n",
      "[12]\ttraining's binary_logloss: 0.250237\tvalid_1's binary_logloss: 0.35776\n",
      "[13]\ttraining's binary_logloss: 0.24118\tvalid_1's binary_logloss: 0.356372\n",
      "[14]\ttraining's binary_logloss: 0.232165\tvalid_1's binary_logloss: 0.354685\n",
      "[15]\ttraining's binary_logloss: 0.223734\tvalid_1's binary_logloss: 0.350611\n",
      "[16]\ttraining's binary_logloss: 0.215222\tvalid_1's binary_logloss: 0.347434\n",
      "[17]\ttraining's binary_logloss: 0.206229\tvalid_1's binary_logloss: 0.340112\n",
      "[18]\ttraining's binary_logloss: 0.199365\tvalid_1's binary_logloss: 0.338613\n",
      "[19]\ttraining's binary_logloss: 0.192896\tvalid_1's binary_logloss: 0.338771\n",
      "[20]\ttraining's binary_logloss: 0.185586\tvalid_1's binary_logloss: 0.335636\n",
      "[21]\ttraining's binary_logloss: 0.179501\tvalid_1's binary_logloss: 0.333815\n",
      "[22]\ttraining's binary_logloss: 0.17259\tvalid_1's binary_logloss: 0.331526\n",
      "[23]\ttraining's binary_logloss: 0.167561\tvalid_1's binary_logloss: 0.332052\n",
      "[24]\ttraining's binary_logloss: 0.161963\tvalid_1's binary_logloss: 0.329641\n",
      "[25]\ttraining's binary_logloss: 0.156694\tvalid_1's binary_logloss: 0.330161\n",
      "[26]\ttraining's binary_logloss: 0.151828\tvalid_1's binary_logloss: 0.330539\n",
      "[27]\ttraining's binary_logloss: 0.147209\tvalid_1's binary_logloss: 0.328629\n",
      "[28]\ttraining's binary_logloss: 0.142055\tvalid_1's binary_logloss: 0.326747\n",
      "[29]\ttraining's binary_logloss: 0.137812\tvalid_1's binary_logloss: 0.324561\n",
      "[30]\ttraining's binary_logloss: 0.133509\tvalid_1's binary_logloss: 0.324969\n",
      "[31]\ttraining's binary_logloss: 0.129279\tvalid_1's binary_logloss: 0.325187\n",
      "[32]\ttraining's binary_logloss: 0.124923\tvalid_1's binary_logloss: 0.323764\n",
      "[33]\ttraining's binary_logloss: 0.120719\tvalid_1's binary_logloss: 0.323174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/boost/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/opt/anaconda3/envs/boost/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34]\ttraining's binary_logloss: 0.11677\tvalid_1's binary_logloss: 0.323868\n",
      "[35]\ttraining's binary_logloss: 0.112699\tvalid_1's binary_logloss: 0.320684\n",
      "[36]\ttraining's binary_logloss: 0.108639\tvalid_1's binary_logloss: 0.322585\n",
      "[37]\ttraining's binary_logloss: 0.105643\tvalid_1's binary_logloss: 0.324125\n",
      "[38]\ttraining's binary_logloss: 0.102629\tvalid_1's binary_logloss: 0.324892\n",
      "[39]\ttraining's binary_logloss: 0.0996575\tvalid_1's binary_logloss: 0.326262\n",
      "[40]\ttraining's binary_logloss: 0.0959901\tvalid_1's binary_logloss: 0.32472\n",
      "[41]\ttraining's binary_logloss: 0.0928156\tvalid_1's binary_logloss: 0.322005\n",
      "[42]\ttraining's binary_logloss: 0.089765\tvalid_1's binary_logloss: 0.322139\n",
      "[43]\ttraining's binary_logloss: 0.0872162\tvalid_1's binary_logloss: 0.321283\n",
      "[44]\ttraining's binary_logloss: 0.0846743\tvalid_1's binary_logloss: 0.321737\n",
      "[45]\ttraining's binary_logloss: 0.081819\tvalid_1's binary_logloss: 0.321651\n",
      "[46]\ttraining's binary_logloss: 0.0794074\tvalid_1's binary_logloss: 0.321132\n",
      "[47]\ttraining's binary_logloss: 0.0769197\tvalid_1's binary_logloss: 0.319822\n",
      "[48]\ttraining's binary_logloss: 0.0743116\tvalid_1's binary_logloss: 0.319052\n",
      "[49]\ttraining's binary_logloss: 0.072199\tvalid_1's binary_logloss: 0.320728\n",
      "[50]\ttraining's binary_logloss: 0.070155\tvalid_1's binary_logloss: 0.321551\n",
      "[51]\ttraining's binary_logloss: 0.0684894\tvalid_1's binary_logloss: 0.322163\n",
      "[52]\ttraining's binary_logloss: 0.0663533\tvalid_1's binary_logloss: 0.321852\n",
      "[53]\ttraining's binary_logloss: 0.0644838\tvalid_1's binary_logloss: 0.321961\n",
      "[54]\ttraining's binary_logloss: 0.0625181\tvalid_1's binary_logloss: 0.321294\n",
      "[55]\ttraining's binary_logloss: 0.0608883\tvalid_1's binary_logloss: 0.323601\n",
      "[56]\ttraining's binary_logloss: 0.0591807\tvalid_1's binary_logloss: 0.326173\n",
      "[57]\ttraining's binary_logloss: 0.0576375\tvalid_1's binary_logloss: 0.326574\n",
      "[58]\ttraining's binary_logloss: 0.0561462\tvalid_1's binary_logloss: 0.324577\n",
      "[59]\ttraining's binary_logloss: 0.0546828\tvalid_1's binary_logloss: 0.325167\n",
      "[60]\ttraining's binary_logloss: 0.0536448\tvalid_1's binary_logloss: 0.326918\n",
      "[61]\ttraining's binary_logloss: 0.0521338\tvalid_1's binary_logloss: 0.32546\n",
      "[62]\ttraining's binary_logloss: 0.0508082\tvalid_1's binary_logloss: 0.3235\n",
      "[63]\ttraining's binary_logloss: 0.0495284\tvalid_1's binary_logloss: 0.324299\n",
      "[64]\ttraining's binary_logloss: 0.0482814\tvalid_1's binary_logloss: 0.323237\n",
      "[65]\ttraining's binary_logloss: 0.0470436\tvalid_1's binary_logloss: 0.324003\n",
      "[66]\ttraining's binary_logloss: 0.0460041\tvalid_1's binary_logloss: 0.324883\n",
      "[67]\ttraining's binary_logloss: 0.0448428\tvalid_1's binary_logloss: 0.325324\n",
      "[68]\ttraining's binary_logloss: 0.0438072\tvalid_1's binary_logloss: 0.32587\n",
      "[69]\ttraining's binary_logloss: 0.0427412\tvalid_1's binary_logloss: 0.326461\n",
      "[70]\ttraining's binary_logloss: 0.041769\tvalid_1's binary_logloss: 0.325737\n",
      "[71]\ttraining's binary_logloss: 0.0408181\tvalid_1's binary_logloss: 0.326975\n",
      "[72]\ttraining's binary_logloss: 0.0397793\tvalid_1's binary_logloss: 0.328202\n",
      "[73]\ttraining's binary_logloss: 0.038925\tvalid_1's binary_logloss: 0.328785\n",
      "[74]\ttraining's binary_logloss: 0.0381072\tvalid_1's binary_logloss: 0.329297\n",
      "[75]\ttraining's binary_logloss: 0.0373042\tvalid_1's binary_logloss: 0.330996\n",
      "[76]\ttraining's binary_logloss: 0.0363881\tvalid_1's binary_logloss: 0.33193\n",
      "[77]\ttraining's binary_logloss: 0.0354791\tvalid_1's binary_logloss: 0.330262\n",
      "[78]\ttraining's binary_logloss: 0.0347303\tvalid_1's binary_logloss: 0.331742\n",
      "[79]\ttraining's binary_logloss: 0.0340579\tvalid_1's binary_logloss: 0.33169\n",
      "[80]\ttraining's binary_logloss: 0.0333481\tvalid_1's binary_logloss: 0.33065\n",
      "[81]\ttraining's binary_logloss: 0.0325833\tvalid_1's binary_logloss: 0.331981\n",
      "[82]\ttraining's binary_logloss: 0.0319964\tvalid_1's binary_logloss: 0.332352\n",
      "[83]\ttraining's binary_logloss: 0.0312556\tvalid_1's binary_logloss: 0.331782\n",
      "[84]\ttraining's binary_logloss: 0.0305442\tvalid_1's binary_logloss: 0.330262\n",
      "[85]\ttraining's binary_logloss: 0.0299611\tvalid_1's binary_logloss: 0.332911\n",
      "[86]\ttraining's binary_logloss: 0.029313\tvalid_1's binary_logloss: 0.33243\n",
      "[87]\ttraining's binary_logloss: 0.0287538\tvalid_1's binary_logloss: 0.33322\n",
      "[88]\ttraining's binary_logloss: 0.0282117\tvalid_1's binary_logloss: 0.332903\n",
      "[89]\ttraining's binary_logloss: 0.0276521\tvalid_1's binary_logloss: 0.331112\n",
      "[90]\ttraining's binary_logloss: 0.0270814\tvalid_1's binary_logloss: 0.332249\n",
      "[91]\ttraining's binary_logloss: 0.0266124\tvalid_1's binary_logloss: 0.332759\n",
      "[92]\ttraining's binary_logloss: 0.026087\tvalid_1's binary_logloss: 0.333162\n",
      "[93]\ttraining's binary_logloss: 0.02557\tvalid_1's binary_logloss: 0.33345\n",
      "[94]\ttraining's binary_logloss: 0.0251236\tvalid_1's binary_logloss: 0.335077\n",
      "[95]\ttraining's binary_logloss: 0.0246248\tvalid_1's binary_logloss: 0.334766\n",
      "[96]\ttraining's binary_logloss: 0.0242205\tvalid_1's binary_logloss: 0.334988\n",
      "[97]\ttraining's binary_logloss: 0.0237185\tvalid_1's binary_logloss: 0.334801\n",
      "[98]\ttraining's binary_logloss: 0.0232693\tvalid_1's binary_logloss: 0.335739\n",
      "[1]\ttraining's binary_logloss: 0.429075\tvalid_1's binary_logloss: 0.43476\n",
      "[2]\ttraining's binary_logloss: 0.404763\tvalid_1's binary_logloss: 0.425998\n",
      "[3]\ttraining's binary_logloss: 0.382944\tvalid_1's binary_logloss: 0.415\n",
      "[4]\ttraining's binary_logloss: 0.361338\tvalid_1's binary_logloss: 0.40572\n",
      "[5]\ttraining's binary_logloss: 0.343348\tvalid_1's binary_logloss: 0.395634\n",
      "[6]\ttraining's binary_logloss: 0.327741\tvalid_1's binary_logloss: 0.388208\n",
      "[7]\ttraining's binary_logloss: 0.313582\tvalid_1's binary_logloss: 0.384037\n",
      "[8]\ttraining's binary_logloss: 0.298332\tvalid_1's binary_logloss: 0.380015\n",
      "[9]\ttraining's binary_logloss: 0.285296\tvalid_1's binary_logloss: 0.37752\n",
      "[10]\ttraining's binary_logloss: 0.272374\tvalid_1's binary_logloss: 0.374382\n",
      "[11]\ttraining's binary_logloss: 0.26023\tvalid_1's binary_logloss: 0.373181\n",
      "[12]\ttraining's binary_logloss: 0.249822\tvalid_1's binary_logloss: 0.372749\n",
      "[13]\ttraining's binary_logloss: 0.238804\tvalid_1's binary_logloss: 0.369639\n",
      "[14]\ttraining's binary_logloss: 0.229408\tvalid_1's binary_logloss: 0.369907\n",
      "[15]\ttraining's binary_logloss: 0.220318\tvalid_1's binary_logloss: 0.368204\n",
      "[16]\ttraining's binary_logloss: 0.212695\tvalid_1's binary_logloss: 0.365811\n",
      "[17]\ttraining's binary_logloss: 0.204453\tvalid_1's binary_logloss: 0.366402\n",
      "[18]\ttraining's binary_logloss: 0.197413\tvalid_1's binary_logloss: 0.365354\n",
      "[19]\ttraining's binary_logloss: 0.190303\tvalid_1's binary_logloss: 0.360906\n",
      "[20]\ttraining's binary_logloss: 0.184005\tvalid_1's binary_logloss: 0.360871\n",
      "[21]\ttraining's binary_logloss: 0.177059\tvalid_1's binary_logloss: 0.355969\n",
      "[22]\ttraining's binary_logloss: 0.170918\tvalid_1's binary_logloss: 0.355411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/boost/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/opt/anaconda3/envs/boost/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23]\ttraining's binary_logloss: 0.165279\tvalid_1's binary_logloss: 0.356668\n",
      "[24]\ttraining's binary_logloss: 0.159768\tvalid_1's binary_logloss: 0.35677\n",
      "[25]\ttraining's binary_logloss: 0.154329\tvalid_1's binary_logloss: 0.35519\n",
      "[26]\ttraining's binary_logloss: 0.149329\tvalid_1's binary_logloss: 0.353475\n",
      "[27]\ttraining's binary_logloss: 0.144201\tvalid_1's binary_logloss: 0.355178\n",
      "[28]\ttraining's binary_logloss: 0.14028\tvalid_1's binary_logloss: 0.354456\n",
      "[29]\ttraining's binary_logloss: 0.13508\tvalid_1's binary_logloss: 0.356339\n",
      "[30]\ttraining's binary_logloss: 0.130166\tvalid_1's binary_logloss: 0.35732\n",
      "[31]\ttraining's binary_logloss: 0.125826\tvalid_1's binary_logloss: 0.360647\n",
      "[32]\ttraining's binary_logloss: 0.122028\tvalid_1's binary_logloss: 0.361094\n",
      "[33]\ttraining's binary_logloss: 0.117958\tvalid_1's binary_logloss: 0.360711\n",
      "[34]\ttraining's binary_logloss: 0.114039\tvalid_1's binary_logloss: 0.360995\n",
      "[35]\ttraining's binary_logloss: 0.110651\tvalid_1's binary_logloss: 0.36287\n",
      "[36]\ttraining's binary_logloss: 0.10738\tvalid_1's binary_logloss: 0.361966\n",
      "[37]\ttraining's binary_logloss: 0.104052\tvalid_1's binary_logloss: 0.36111\n",
      "[38]\ttraining's binary_logloss: 0.101401\tvalid_1's binary_logloss: 0.363154\n",
      "[39]\ttraining's binary_logloss: 0.0981768\tvalid_1's binary_logloss: 0.362454\n",
      "[40]\ttraining's binary_logloss: 0.0950191\tvalid_1's binary_logloss: 0.366643\n",
      "[41]\ttraining's binary_logloss: 0.0925241\tvalid_1's binary_logloss: 0.370006\n",
      "[42]\ttraining's binary_logloss: 0.0896728\tvalid_1's binary_logloss: 0.368223\n",
      "[43]\ttraining's binary_logloss: 0.0870598\tvalid_1's binary_logloss: 0.368259\n",
      "[44]\ttraining's binary_logloss: 0.0843976\tvalid_1's binary_logloss: 0.368163\n",
      "[45]\ttraining's binary_logloss: 0.0822759\tvalid_1's binary_logloss: 0.369319\n",
      "[46]\ttraining's binary_logloss: 0.0798858\tvalid_1's binary_logloss: 0.369435\n",
      "[47]\ttraining's binary_logloss: 0.0781141\tvalid_1's binary_logloss: 0.374298\n",
      "[48]\ttraining's binary_logloss: 0.0760546\tvalid_1's binary_logloss: 0.374819\n",
      "[49]\ttraining's binary_logloss: 0.0741009\tvalid_1's binary_logloss: 0.377119\n",
      "[50]\ttraining's binary_logloss: 0.072069\tvalid_1's binary_logloss: 0.375421\n",
      "[51]\ttraining's binary_logloss: 0.070321\tvalid_1's binary_logloss: 0.37598\n",
      "[52]\ttraining's binary_logloss: 0.0681782\tvalid_1's binary_logloss: 0.375766\n",
      "[53]\ttraining's binary_logloss: 0.0665625\tvalid_1's binary_logloss: 0.378229\n",
      "[54]\ttraining's binary_logloss: 0.0647261\tvalid_1's binary_logloss: 0.37774\n",
      "[55]\ttraining's binary_logloss: 0.0630598\tvalid_1's binary_logloss: 0.379555\n",
      "[56]\ttraining's binary_logloss: 0.061439\tvalid_1's binary_logloss: 0.381392\n",
      "[57]\ttraining's binary_logloss: 0.0595306\tvalid_1's binary_logloss: 0.383731\n",
      "[58]\ttraining's binary_logloss: 0.0578597\tvalid_1's binary_logloss: 0.383036\n",
      "[59]\ttraining's binary_logloss: 0.0562194\tvalid_1's binary_logloss: 0.387694\n",
      "[60]\ttraining's binary_logloss: 0.0546557\tvalid_1's binary_logloss: 0.390929\n",
      "[61]\ttraining's binary_logloss: 0.0531421\tvalid_1's binary_logloss: 0.392361\n",
      "[62]\ttraining's binary_logloss: 0.0516515\tvalid_1's binary_logloss: 0.393255\n",
      "[63]\ttraining's binary_logloss: 0.0503153\tvalid_1's binary_logloss: 0.395354\n",
      "[64]\ttraining's binary_logloss: 0.0492026\tvalid_1's binary_logloss: 0.395872\n",
      "[65]\ttraining's binary_logloss: 0.0480751\tvalid_1's binary_logloss: 0.397361\n",
      "[66]\ttraining's binary_logloss: 0.0471731\tvalid_1's binary_logloss: 0.400359\n",
      "[67]\ttraining's binary_logloss: 0.0461125\tvalid_1's binary_logloss: 0.403182\n",
      "[68]\ttraining's binary_logloss: 0.0448667\tvalid_1's binary_logloss: 0.406322\n",
      "[69]\ttraining's binary_logloss: 0.0437852\tvalid_1's binary_logloss: 0.407389\n",
      "[70]\ttraining's binary_logloss: 0.0426892\tvalid_1's binary_logloss: 0.40813\n",
      "[71]\ttraining's binary_logloss: 0.0416278\tvalid_1's binary_logloss: 0.408169\n",
      "[72]\ttraining's binary_logloss: 0.0406627\tvalid_1's binary_logloss: 0.410312\n",
      "[73]\ttraining's binary_logloss: 0.0398251\tvalid_1's binary_logloss: 0.412173\n",
      "[74]\ttraining's binary_logloss: 0.0388504\tvalid_1's binary_logloss: 0.415412\n",
      "[75]\ttraining's binary_logloss: 0.0379813\tvalid_1's binary_logloss: 0.416812\n",
      "[76]\ttraining's binary_logloss: 0.0370827\tvalid_1's binary_logloss: 0.417566\n",
      "[1]\ttraining's binary_logloss: 0.426634\tvalid_1's binary_logloss: 0.436124\n",
      "[2]\ttraining's binary_logloss: 0.400601\tvalid_1's binary_logloss: 0.424927\n",
      "[3]\ttraining's binary_logloss: 0.379326\tvalid_1's binary_logloss: 0.413629\n",
      "[4]\ttraining's binary_logloss: 0.359966\tvalid_1's binary_logloss: 0.403044\n",
      "[5]\ttraining's binary_logloss: 0.343019\tvalid_1's binary_logloss: 0.396469\n",
      "[6]\ttraining's binary_logloss: 0.326411\tvalid_1's binary_logloss: 0.393221\n",
      "[7]\ttraining's binary_logloss: 0.312928\tvalid_1's binary_logloss: 0.388358\n",
      "[8]\ttraining's binary_logloss: 0.299965\tvalid_1's binary_logloss: 0.38277\n",
      "[9]\ttraining's binary_logloss: 0.287283\tvalid_1's binary_logloss: 0.377091\n",
      "[10]\ttraining's binary_logloss: 0.273966\tvalid_1's binary_logloss: 0.375025\n",
      "[11]\ttraining's binary_logloss: 0.264808\tvalid_1's binary_logloss: 0.372372\n",
      "[12]\ttraining's binary_logloss: 0.254353\tvalid_1's binary_logloss: 0.368294\n",
      "[13]\ttraining's binary_logloss: 0.244249\tvalid_1's binary_logloss: 0.364038\n",
      "[14]\ttraining's binary_logloss: 0.234855\tvalid_1's binary_logloss: 0.367709\n",
      "[15]\ttraining's binary_logloss: 0.226321\tvalid_1's binary_logloss: 0.363639\n",
      "[16]\ttraining's binary_logloss: 0.218283\tvalid_1's binary_logloss: 0.364689\n",
      "[17]\ttraining's binary_logloss: 0.21036\tvalid_1's binary_logloss: 0.363605\n",
      "[18]\ttraining's binary_logloss: 0.203483\tvalid_1's binary_logloss: 0.362448\n",
      "[19]\ttraining's binary_logloss: 0.194902\tvalid_1's binary_logloss: 0.357725\n",
      "[20]\ttraining's binary_logloss: 0.188911\tvalid_1's binary_logloss: 0.357373\n",
      "[21]\ttraining's binary_logloss: 0.182618\tvalid_1's binary_logloss: 0.359371\n",
      "[22]\ttraining's binary_logloss: 0.175086\tvalid_1's binary_logloss: 0.356985\n",
      "[23]\ttraining's binary_logloss: 0.168531\tvalid_1's binary_logloss: 0.354108\n",
      "[24]\ttraining's binary_logloss: 0.162044\tvalid_1's binary_logloss: 0.352017\n",
      "[25]\ttraining's binary_logloss: 0.156274\tvalid_1's binary_logloss: 0.353524\n",
      "[26]\ttraining's binary_logloss: 0.150571\tvalid_1's binary_logloss: 0.350859\n",
      "[27]\ttraining's binary_logloss: 0.145861\tvalid_1's binary_logloss: 0.349023\n",
      "[28]\ttraining's binary_logloss: 0.140678\tvalid_1's binary_logloss: 0.351417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/boost/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/opt/anaconda3/envs/boost/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29]\ttraining's binary_logloss: 0.136293\tvalid_1's binary_logloss: 0.351764\n",
      "[30]\ttraining's binary_logloss: 0.13103\tvalid_1's binary_logloss: 0.352691\n",
      "[31]\ttraining's binary_logloss: 0.127303\tvalid_1's binary_logloss: 0.350706\n",
      "[32]\ttraining's binary_logloss: 0.123225\tvalid_1's binary_logloss: 0.351529\n",
      "[33]\ttraining's binary_logloss: 0.118673\tvalid_1's binary_logloss: 0.353358\n",
      "[34]\ttraining's binary_logloss: 0.11465\tvalid_1's binary_logloss: 0.351615\n",
      "[35]\ttraining's binary_logloss: 0.111265\tvalid_1's binary_logloss: 0.352435\n",
      "[36]\ttraining's binary_logloss: 0.107615\tvalid_1's binary_logloss: 0.355422\n",
      "[37]\ttraining's binary_logloss: 0.104333\tvalid_1's binary_logloss: 0.352548\n",
      "[38]\ttraining's binary_logloss: 0.101242\tvalid_1's binary_logloss: 0.350577\n",
      "[39]\ttraining's binary_logloss: 0.0974788\tvalid_1's binary_logloss: 0.351214\n",
      "[40]\ttraining's binary_logloss: 0.0944755\tvalid_1's binary_logloss: 0.351382\n",
      "[41]\ttraining's binary_logloss: 0.0915651\tvalid_1's binary_logloss: 0.35283\n",
      "[42]\ttraining's binary_logloss: 0.0887997\tvalid_1's binary_logloss: 0.353585\n",
      "[43]\ttraining's binary_logloss: 0.0862012\tvalid_1's binary_logloss: 0.35308\n",
      "[44]\ttraining's binary_logloss: 0.0837628\tvalid_1's binary_logloss: 0.353394\n",
      "[45]\ttraining's binary_logloss: 0.0812547\tvalid_1's binary_logloss: 0.35218\n",
      "[46]\ttraining's binary_logloss: 0.0785844\tvalid_1's binary_logloss: 0.353075\n",
      "[47]\ttraining's binary_logloss: 0.0761046\tvalid_1's binary_logloss: 0.356151\n",
      "[48]\ttraining's binary_logloss: 0.0740145\tvalid_1's binary_logloss: 0.356981\n",
      "[49]\ttraining's binary_logloss: 0.0720109\tvalid_1's binary_logloss: 0.356835\n",
      "[50]\ttraining's binary_logloss: 0.0700826\tvalid_1's binary_logloss: 0.354363\n",
      "[51]\ttraining's binary_logloss: 0.0682538\tvalid_1's binary_logloss: 0.356514\n",
      "[52]\ttraining's binary_logloss: 0.0665426\tvalid_1's binary_logloss: 0.356515\n",
      "[53]\ttraining's binary_logloss: 0.06453\tvalid_1's binary_logloss: 0.358288\n",
      "[54]\ttraining's binary_logloss: 0.0625491\tvalid_1's binary_logloss: 0.356396\n",
      "[55]\ttraining's binary_logloss: 0.0607766\tvalid_1's binary_logloss: 0.356885\n",
      "[56]\ttraining's binary_logloss: 0.0591091\tvalid_1's binary_logloss: 0.359762\n",
      "[57]\ttraining's binary_logloss: 0.0574803\tvalid_1's binary_logloss: 0.359718\n",
      "[58]\ttraining's binary_logloss: 0.0559312\tvalid_1's binary_logloss: 0.36166\n",
      "[59]\ttraining's binary_logloss: 0.0543491\tvalid_1's binary_logloss: 0.362335\n",
      "[60]\ttraining's binary_logloss: 0.0529492\tvalid_1's binary_logloss: 0.363354\n",
      "[61]\ttraining's binary_logloss: 0.0516114\tvalid_1's binary_logloss: 0.365431\n",
      "[62]\ttraining's binary_logloss: 0.0504532\tvalid_1's binary_logloss: 0.366047\n",
      "[63]\ttraining's binary_logloss: 0.0492768\tvalid_1's binary_logloss: 0.365513\n",
      "[64]\ttraining's binary_logloss: 0.0478404\tvalid_1's binary_logloss: 0.366301\n",
      "[65]\ttraining's binary_logloss: 0.0465405\tvalid_1's binary_logloss: 0.365905\n",
      "[66]\ttraining's binary_logloss: 0.0454315\tvalid_1's binary_logloss: 0.367188\n",
      "[67]\ttraining's binary_logloss: 0.0443435\tvalid_1's binary_logloss: 0.367498\n",
      "[68]\ttraining's binary_logloss: 0.0431971\tvalid_1's binary_logloss: 0.367577\n",
      "[69]\ttraining's binary_logloss: 0.0421269\tvalid_1's binary_logloss: 0.369087\n",
      "[70]\ttraining's binary_logloss: 0.0412457\tvalid_1's binary_logloss: 0.367994\n",
      "[71]\ttraining's binary_logloss: 0.0403528\tvalid_1's binary_logloss: 0.368021\n",
      "[72]\ttraining's binary_logloss: 0.0395688\tvalid_1's binary_logloss: 0.368969\n",
      "[73]\ttraining's binary_logloss: 0.0385605\tvalid_1's binary_logloss: 0.370888\n",
      "[74]\ttraining's binary_logloss: 0.0376164\tvalid_1's binary_logloss: 0.372889\n",
      "[75]\ttraining's binary_logloss: 0.0367495\tvalid_1's binary_logloss: 0.372652\n",
      "[76]\ttraining's binary_logloss: 0.0359635\tvalid_1's binary_logloss: 0.373545\n",
      "[77]\ttraining's binary_logloss: 0.0350288\tvalid_1's binary_logloss: 0.37413\n",
      "Best iteration for each round: [19, 33, 48, 26, 27]; mean=30.6\n"
     ]
    }
   ],
   "source": [
    "max_iterator = 500   # num of max trees\n",
    "lgb_params = ml.vote_hyperparam(cv_lgb.cv_results_, top_n=7)  # top 7 mods similar performance\n",
    "best_it = []\n",
    "# lgb_params[\"num_leaves\"] = 20 # to address overfitting\n",
    "for i in range(5):\n",
    "    lgb_train, lgb_validate, o_train, o_validate = train_test_split(\n",
    "        x_train, y_train, train_size=0.8, stratify=y_train, random_state = i*10)\n",
    "    mod_lgb = lgb.LGBMClassifier(boosting_type='gbdt', objective='binary', \n",
    "                                eval_metric = 'logloss',\n",
    "                                random_state=i, early_stopping_rounds = max_iterator//10,\n",
    "                                n_estimators= max_iterator, \n",
    "                                **lgb_params  # use ** to unpack dict\n",
    "                               ) \n",
    "    mod_lgb.fit(lgb_train, o_train,\n",
    "                eval_set=[(lgb_train, o_train), (lgb_validate, o_validate)]) \n",
    "    best_it.append(mod_lgb.best_iteration_)\n",
    "print(f\"Best iteration for each round: {best_it}; mean={sum(best_it)/5}\")\n",
    "lgb_it = round(sum(best_it)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e68f6334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/boost/lib/python3.8/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/anaconda3/envs/boost/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/opt/anaconda3/envs/boost/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fee2b15fca0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# final model\n",
    "\n",
    "mod_lgb = lgb.LGBMClassifier(boosting_type='gbdt', objective='binary', \n",
    "                                eval_metric = 'logloss',\n",
    "                                random_state=77,\n",
    "                                n_estimators= lgb_it, \n",
    "                                **lgb_params  # use ** to unpack dict\n",
    "                               ) \n",
    "mod_lgb.fit(x_train, y_train, verbose=0,\n",
    "            eval_set=[(x_train, y_train), (x_test, y_test)])\n",
    "results = mod_lgb.evals_result_\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(results[\"training\"][\"binary_logloss\"], label=\"Training loss\")\n",
    "plt.plot(results[\"valid_1\"][\"binary_logloss\"], label=\"Testing loss\")\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a5a1f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lgb = {'train_pred':mod_lgb.predict(x_train), 'train_prob':mod_lgb.predict_proba(x_train)[:,1],\n",
    "  'test_pred':mod_lgb.predict(x_test), 'test_prob':mod_lgb.predict_proba(x_test)[:,1]}\n",
    "dc.save_py(result_lgb, \"output/result_lgb_imp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d5b7c392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8863"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mod_lgb.predict_proba(x_test)[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "auc = round(metrics.roc_auc_score(y_test, y_pred), 4)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889bd6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
